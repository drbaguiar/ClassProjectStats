rm(datasub)
rm(datetime)
source('~/GitHub/ClassProjectExData/plot2.R')
source('~/GitHub/ClassProjectExData/plot4.R')
source('~/GitHub/ClassProjectExData/plot3.R')
ls()
len(datasub)
length(datasub)
source('~/GitHub/ClassProjectExData/plot4.R')
source('~/GitHub/ClassProjectExData/plot4.R')
dim(x)
ls()
source('~/GitHub/ClassProjectExData/plot1.R')
dim(datasub)
names(datetime)
names(datasub)
dim(datetime)
ses
maj
x
?rug
names(datasub)
hist(globalreactivepower)
hist(datasub$globalreactivepower)
hist(datasub$globalactivepower)
rug
rug(datasub$globalactivepower)
gss <- read.table("./data/gss.csv", header = TRUE, sep =",")
gss <- read.table("./data/gss.csv", header = TRUE, sep =",")
library(car)
Anova(mpg~weight)
apply(mtcars)
mtcars
Anova(mpg~cyl, data=mtcars)
library(faraway)
data(coagulation, package="faraway")
coagulation
plot(coag ~ diet, coagulation,ylab="coagulation time")
stripchart(coag ~ diet, coagulation, vertical=TRUE, method="stack",xlab="diet",ylab="coagulation time")
library(faraway)
installpackages("faraway")
install.packages("faraway")
library(faraway)
data(coagulation, package="faraway")
coagulation
plot(coag ~ diet, coagulation,ylab="coagulation time")
stripchart(coag ~ diet, coagulation, vertical=TRUE, method="stack",xlab="diet",ylab="coagulation time")
gss <- read.table("./data/gss.csv", header = TRUE, sep =",")
gss <- read.table("./data/gss.csv", header = TRUE, sep =",")
install.packages("nortest")
library(nortest)
adTest(rnorm(100, mean = 5, sd = 3))
ad.test(rnorm(100, mean = 5, sd = 3))
adTest(runif(100, min = 2, max = 4))
ad.test(runif(100, min = 2, max = 4))
gss <- read.table("./data/gss.csv", header = TRUE, sep =",")
require("lawstat")
install.packages("lawstat")
install.packages(c("aplpack", "data.table", "DBI", "devtools", "dplyr", "jsonlite", "knitr", "mime", "multcomp", "quantmod", "Rcmdr", "Rcpp", "rgl", "testthat"))
install.packages("data,tables")
install.packages("data.tables")
install.packages("data.tables")
install.packages("data,table")
install.packages("data.table")
x <- c(
## Data Import and Manipulation
'XML', # tools for parsing and generating XML
'foreign', # functions for reading and writing data stored by Minitab, S, SAS, SPSS...
'lubridate', # makes it easier to work with dates and times by providing functions to identify and parse date-time data
'stringr', # makes it easier to work with strings
'sqldf', # for running SQL statements on R data frames, optimized for convenience
'RCurl', # general network (HTTP/FTP/...) client interface for R
'rjson', # converts R object into JSON objects and vice-versa
'xlsx', # provides R functions to read/write/format Excel 2007 and Excel 97/2000/XP/2003 file formats
'tidyr', # an evolution of reshape2. It's designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines
'dplyr', # a fast, consistent tool for working with data frame like objects, both in memory and out of memory
'httr', # provides useful tools for working with HTTP
'data.table', # Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all
## Exploratory Data Analysis and Visualization
'ggplot2', # an implementation of the Grammar of Graphics
'RColorBrewer', # provides palettes for drawing nice maps shaded according to a variable
'ellipse', # functions for drawing ellipses and ellipse-like confidence regions
'animation', # a gallery of animations in statistics and utilities to create animations
'mcmcplots', # for visual diagnostics of posterior samples
'shiny', # elegant and powerful web framework for building interactive web applications using R
## Machine Learning and Statistical Modeling
'tree', # classification and regression trees
'randomForest', # classification and regression based on a forest of trees using random inputs
'tm', # a framework for text mining applications within R
'mclust', # Normal Mixture Modeling for Model-Based Clustering, Classification, and Density Estimation
'car', # Companion to Applied Regression. Esp. useful for ANOVA tables.
'arm', # functions for processing lm, glm, svy.glm, mer and polr outputs
'gbm', # Generalized Boosted Regression Models
'glmnet', # lasso and elastic-net regularized generalized linear models
'lme4', # linear mixed-effects models using S4 classes
'mvtnorm', # multivariate Normal and t Distributions
'sde', # Simulation and Inference for Stochastic Differential Equations
'coda', # Output analysis and diagnostics for Markov Chain Monte Carlo simulations
'lda', # Collapsed Gibbs sampling methods for topic models. This package implements latent Dirichlet allocation (LDA) and related models.
'forecast', # for easy forecasting of time series,
'qcc', # statistical quality control and QC charts
'mice', # multiple imputation using Fully Conditional Specification (FCS)
## Reporting
'xtable', # Export tables to LaTeX or HTML
'knitr', #  A general-purpose package for dynamic report generation in R
'stargazer', # LaTeX/HTML code and ASCII text for well-formatted regression and summary statistics tables
## Miscellaneous
# 'sendmailR', # send email using R
'devtools', # Tools to make developing R code easier
'gridExtra', # misc. high-level Grid functions
'twitteR', # R based Twitter client
'doMC', # for multi-core processing
'packrat' # Reproducible package management for R, more info @ http://rstudio.github.io/packrat/
)
install.packages(x)
rm(x)
install.packages(rattle, dependencies = c("Depends", "Suggests"))
install.packages("rattle")
install.packages("twitteR")
install.packages("twitteR")
###
### Read tweets from Twitter using ATOM (XML) format
###
# installation is required only required once and is rememberd across sessions
install.packages('XML')
# loading the package is required once each session
require(XML)
# initialize a storage variable for Twitter tweets
mydata.vectors <- character(0)
# paginate to get more tweets
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
# how many tweets did we get?
length(mydata.vectors)
###
### Read tweets from Twitter using ATOM (XML) format
###
# installation is required only required once and is rememberd across sessions
install.packages('XML')
# loading the package is required once each session
require(XML)
# initialize a storage variable for Twitter tweets
mydata.vectors <- character(0)
# paginate to get more tweets
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
# how many tweets did we get?
length(mydata.vectors)
install.packages("XML")
# loading the package is required once each session
require(XML)
# initialize a storage variable for Twitter tweets
mydata.vectors <- character(0)
# paginate to get more tweets
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
# how many tweets did we get?
length(mydata.vectors)
mydata.vectors <- character(0)
twitter_q <- URLencode('#prolife OR #prochoice')
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
library(nycflights13)
install.packages("nycflights13")
library(nycflights13)
attach(flights)
filter(flights, month == 1, day == 1)
filter(flights, month == 1)
library(dplyr)
filter(flights, month == 1, day == 1)
summarise(flights,
delay = mean(dep_delay, na.rm = TRUE))
sample_n(flights, 10)
library(twitteR)
rdmTweets <- userTimeline("rdatamining", n=100)
setup_twitter_oauth("API key", "API secret")
library(twitteR)
setup_twitter_oauth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
?twitteR
load("C:/Users/bryan_000/Downloads/rdmTweets-201306.RData")
library(httr)
setup_twitter_oauth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
setup_twitteR_oauth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
twitteR_oauth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
getTwitterOAuth(consumer_key, consumer_secret)
registerTwitterOAuth(oauth)
getTwitterOAuth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
getTwitterOAuth("79261207-rMLPaWDKKkPg7iwjrNHq0yVX9vEKtYkSVGh7CAluj", "JgFqyPrKcqodyRZ1ApxuzOeTXfrlj7fQpcoX9FXF21Ndt")
getTwitterOAuth("79261207-rMLPaWDKKkPg7iwjrNHq0yVX9vEKtYkSVGh7CAluj", "JgFqyPrKcqodyRZ1ApxuzOeTXfrlj7fQpcoX9FXF21Ndt")
getTwitterOAuth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
getTwitterOAuth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
cred <- OAuthFactory$new(consumerKey='cred <- OAuthFactory$new(consumerKey='secret',
consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='http://api.twitter.com/oauth/access_token',
authURL='http://api.twitter.com/oauth/authorize')',
consumerSecret='secret',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='http://api.twitter.com/oauth/access_token',
authURL='http://api.twitter.com/oauth/authorize')
cred <- OAuthFactory$new(consumerKey='cred <- OAuthFactory$new(consumerKey='secret',
consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='http://api.twitter.com/oauth/access_token',
authURL='http://api.twitter.com/oauth/authorize')
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='http://api.twitter.com/oauth/access_token',authURL='http://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
8461425
cred$handshake(cainfo="cacert.pem")
#install the necessary packages
install.packages("ROAuth")
install.packages("twitteR")
install.packages("wordcloud")
install.packages("tm")
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
#necessary step for Windows
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
#to get your consumerKey and consumerSecret see the twitteR documentation for instructions
cred <- OAuthFactory$new(consumerKey='secret',
consumerSecret='secret',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='http://api.twitter.com/oauth/access_token',
authURL='http://api.twitter.com/oauth/authorize')
#necessary step for Windows
cred$handshake(cainfo="cacert.pem")
#save for later use for Windows
save(cred, file="twitter authentication.Rdata")
registerTwitterOAuth(cred)
#the cainfo parameter is necessary on Windows
r_stats<- searchTwitter("#Rstats", n=1500, cainfo="cacert.pem")
#save text
r_stats_text <- sapply(r_stats, function(x) x$getText())
#create corpus
r_stats_text_corpus <- Corpus(VectorSource(r_stats_text))
#clean up
r_stats_text_corpus <- tm_map(r_stats_text_corpus, tolower)
r_stats_text_corpus <- tm_map(r_stats_text_corpus, removePunctuation)
r_stats_text_corpus <- tm_map(r_stats_text_corpus, function(x)removeWords(x,stopwords()))
wordcloud(r_stats_text_corpus)
install.packages("twitteR")
save(cred, file="twitter authentication.Rdata")
install.packages("twitteR")
install.packages("twitteR")
install.packages("twitteR")
save(cred, file="twitter authentication.Rdata")
registerTwitterOAuth(cred)
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='http://api.twitter.com/oauth/access_token',authURL='http://api.twitter.com/oauth/authorize')
library(twitteR)
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='http://api.twitter.com/oauth/access_token',authURL='http://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
twitCred = c(consumerKey = ' jYEGXw87lkgjitQqUIBVcnUzE', consumerSecret = ' oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',oauthKey = ' 79261207-rMLPaWDKKkPg7iwjrNHq0yVX9vEKtYkSVGh7CAluj',oauthSecret = ' JgFqyPrKcqodyRZ1ApxuzOeTXfrlj7fQpcoX9FXF21Ndt')
curl = getCurlHandle()
options(RCurlOptions = list(capath = system.file('CurlSSL', 'cacert.pem',
package = 'RCurl'), ssl.verifypeer = FALSE))
curlSetOpt(.opts = list(proxy = 'proxyserver:port'), curl = curl)
library("twitteR")
library("ROAuth")
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='http://api.twitter.com/oauth/access_token',authURL='http://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
save(cred, file="twitter authentication.Rdata")
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='https://api.twitter.com/oauth/access_token',authURL='https://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
search.string <- "#nba"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
cred$handshake(cainfo="cacert.pem")
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "jYEGXw87lkgjitQqUIBVcnUzE"
consumerSecret <- "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm"
twitCred <- OAuthFactory$new(consumerKey=consumerKey,consumerSecret=consumerSecret,requestURL=reqURL,accessURL=accessURL,authURL=authURL)
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
registerTwitterOAuth(twitCred)
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
tweets
source('~/GitHub/MyWork/TwitterLogin.R')
registerTwitterOAuth(twitCred)
search.string <- "#highered"; no.of.tweets <- 100; tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en");tweets
source('~/GitHub/MyWork/TwitterGetTweets.R')
tweets
un.tweets = searchTwitter('@United',n=1000, cainfo="cacert.pem")
length(un.tweets)
un.tweets
rm(tweets)
library(plyr)
un.text = laply(un.tweets, function(t)t$getText())
un.text
tr <-getTrends(’weekly’, as.character(Sys.Date()-1))
tr <-getTrends(’weekly’, as.character(Sys.Date()-1))
library(tm)
data.corpus <- Corpus(VectorSource(un.text))
data.corpus <- tm_map(data.corpus, tolower)
data.corpus <- t_map(data.corpus, removePunctuation)
# remove stop words, otherwise some of these words would appear as most used
> some_stopwords <- c(stopwords('english'))
> data.corpus <- tm_map(data.corpus, removeWords, some_stopwords)
some_stopwords <- c(stopwords('english'))
data.corpus <- tm_map(data.corpus, removeWords, some_stopwords)
data.corpus
data.corpus <- TermDocumentMatrix(data.corpus)
data.corpus
data.tm
inspect(data.corpus[1:1000])
un.text
inspect(data.corpus[1:1000])
data.corpus <- t_map(data.corpus, removePunctuation)
library(tm)
install.packages("tm")
library(tm)
data.corpus <- t_map(data.corpus, removePunctuation)
library(TwitteR)
library(twitteR)
data.corpus <- t_map(data.corpus, removePunctuation)
library(plyr)
data.corpus <- t_map(data.corpus, removePunctuation)
data.corpus <- tm_map(data.corpus, removePunctuation)
data.corpus <- tm_map(data.corpus, tolower)
inspect(data.corpus[1:1000])
data.corpus <- TermDocumentMatrix(data.corpus)
data.corpus <- tm_map(data.corpus, removeWords, some_stopwords)
data.corpus
data.tm
data.corpus [[116]]
findFreqTerms(data.dtm, lowfreq=30)
findFreqTerms(data.corpus, lowfreq=30)
tr <- getTrends(’weekly’, as.character(Sys.Date()-1))
tr <- getTrends(’weekly’, as.character(Sys.Date()-1))
data.dtm <- TermDocumentMatrix(data.corpus)
delta.tweets <- searchTwitter('@delta', n=1000)
source('~/GitHub/MyWork/TwitterGetTweets.R')
load("~/GitHub/R/twitter authentication.Rdata")
source('~/GitHub/MyWork/TwitterGetTweets.R')
source('~/GitHub/MyWork/TwitterGetTweets.R')
source('~/GitHub/MyWork/TwitterLogin.R')
source('~/GitHub/MyWork/TwitterGetTweets.R')
tweets
source('~/GitHub/MyWork/TwitterGetTweets.R')
USArrests
library(psych)
fit <- principal(USArrests, nfactors=, rotate="varimax")
fit
install.packages("nFactors")
?pa
?fa
Harman74
require(stats)
Harman74
Harman74.cor
install.packages("FactoMineR")
data(us.bis.yield)
library("Rsafd")
install.package("Rsafd")
install.packages("Rsafd")
install.packages("Rsafd")
data(us.bis.yield)
gender = rep(c("female","male"),c(1835,2691))
admitted = rep(c("yes","no","yes","no"),c(557,1278,1198,1493))
dept = rep(c("A","B","C","D","E","F","A","B","C","D","E","F"),
c(89,17,202,131,94,24,19,8,391,244,299,317))
dept2 = rep(c("A","B","C","D","E","F","A","B","C","D","E","F"),
c(512,353,120,138,53,22,313,207,205,279,138,351))
department = c(dept,dept2)
ucb = data.frame(gender,admitted,department)
rm(gender,admitted,dept,dept2,department)
ls()
rm(accessURL)
rm(cred)
rm(un.text)
rm(un.tweets)
ls()
rm(authURL)
rm(reqURL)
rm(consumerKey)
rm(curl)
rm(no.of.tweets)
rm(reqURL)
ls()
rm(data.corpus)
rm(search.string)
rm(some_stopwords)
rm(tweets)
rm(twitCred)
rm(consumerSecret)
ucb
save(ucb)
?save
save(ucb, file = "ucb.RData")
str(ucb)
summary(ucb)
table(ucb$gender)
table(ucb$admitted)
table(ucb$department) -> dept.table
prop.table(dept.table)
prop.table(dept.table) * 100
with(ucb, table(gender, admitted))
xtabs(~ gender + admitted, data=ucb)
xtabs(~ gender + admitted, data=ucb) -> gen.adm.table
data(faithful)
stem(waiting)
stem(faithful$waiting)
as.data.frame(state.x77) -> states
states$region <- state.region
by(data=states[4], IND=states[9], FUN=mean)
gender = rep(c("female","male"),c(1835,2691))
rm(list=ls())
install.packages("RFacebook")
install.packages("Rfacebook")
install.packages("Rook")
Require(Rfacebook)
require(Rfacebook)
require(Rook)
fb_oauth <- fbOAuth(app_id="176988012511120", app_secret="991efd8aba85c69235c4431b57dc12b8")
fb_oauth <- fbOAuth(app_id="176988012511120", app_secret="CAACEdEose0cBAAKXSOJVin1FLfuMBrzDVxXX7zPNHm28NAA2izuTKlN41zg9Xd7202qT6wKG7gwcn5ViI9qhm20WQQZCdwKNXPLJn7lNRaxFG46RAWo9NpAtwTgk28Nh3fITiN7cKv2g7vHmtkbDmowBk11oS37ZBg9mX2mzuMzBYDt0vz14jmnhaiJQKOvEZATGOlnftKquXZBqAxI8vRtbcpYmU24ZD")
fb_oauth <- fbOAuth(app_id="145634995501895", app_secret="CCAACEdEose0cBAAhAlvU0R49tOx5CZAj9zdXOmo5F54Ks869XdzuNKov9GeiQrCwnPO0kuSSte6WDZAu9jGNxayoo4wn6Ery9hDv6pMSDam9OM0CRpPw2obrNHtZAz01BMwrUQntEgwZCsfebacN4hcXvniYqkJItD0OF9ZC1ZB4Mcg39GfudgMH46ZBiOFVWZB6RbDUHkZBT0EoVnle00Nb3IK3OlZBZBj47w8ZD")
me <- getUsers("me", token=fb_oauth)
fb_oauth <- fbOAuth(app_id="145634995501895", app_secret="CCAACEdEose0cBAAhAlvU0R49tOx5CZAj9zdXOmo5F54Ks869XdzuNKov9GeiQrCwnPO0kuSSte6WDZAu9jGNxayoo4wn6Ery9hDv6pMSDam9OM0CRpPw2obrNHtZAz01BMwrUQntEgwZCsfebacN4hcXvniYqkJItD0OF9ZC1ZB4Mcg39GfudgMH46ZBiOFVWZB6RbDUHkZBT0EoVnle00Nb3IK3OlZBZBj47w8ZD")
token = "CAACEdEose0cBAKTTc22dkBPzz2Bj6ZAOprah478cKIRSYMKLXllIwtTBDUYk9gZABmaxHUDPLBRsLirVCGIaj7FnW0QZA2PbZCiZBZBFwnpztr2Man37wyqgngwYdKyK2jW3dyCnlMFJIe5ZAf91UVPzxKOmnlGdoUZCRLlM7QGAVdOPlJZCmGiUSTdghyPqQYN5IJdKrRs0WEGexjOIjTn0qDNZC9vPHkUZAkZD"
me <- getUsers("me", token=token)
me
my_friends <- getFriends(token, simplify=TRUE)
my_friends_info <- getUsers(my_friends$id, token=token, private_info=TRUE)
table(my_friends_info$location)
my_friends
my_friends <- getFriends(token)
str(me)
str(my_friends)
getFriends(token, simplify = FALSE)
rm(list(ls()))
rm(list(ls())
)
rm(list=ls())
list=ls()
rm(list=ls())
data(mtcars)
str(mtcars)
bestmodel <- step(inclusivemodel, direction = "both")
summary(bestmodel)
inclusivemodel <- lm(mpg ~ ., data = mtcars)
bestmodel <- step(inclusivemodel, direction = "both")
summary(bestmodel)
plot(bestmodel$residualks)
plot(bestmodel$residuals)
abline(o)
abline(0)
abline(0,0)
hist(bestmodel$residuals)
qqnorm(bestmodel$residuals)
abline(bestmodel$residuals)
qqplot(bestmodel$residuals)
qqline(bestmodel$residuals,)
plot(bestmodel$residuals~bestmodel$fitted)
abline(0,0)
plot(abs(bestmodel$residuals)~bestmodel$fitted)
plot(bestmodel$residuals~mtcars$am)
plot(bestmodel$residuals~mtcars$wt)
plot(bestmodel$residuals~mtcars$qsec)
abline(0,0)
plot(bestmodel$residuals~mtcars$am)
plot(bestmodel$residuals~mtcars$am)
gss <- read.table("./data/gss.csv", header = TRUE, sep =",")
getwd()
setwd("C:/Users/bryan_000/Documents/GitHub/ClassProjectStats")
gss <- read.table("./data/gss.csv", header = TRUE, sep =",")
rm(list=ls())
gss <- read.table("./data/gss.csv", header = TRUE, sep =",")
head(gss,35)
gss <- subset(gss, select=c("educ","xmarsex"))
gss <- na.omit(gss)
head(gss,35)
